{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split \n",
    "    #to split the dataset into training set an testing set\n",
    "from sklearn import metrics\n",
    "    #to calculate accuracy and precision of classification\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used the `cars` dataset available from UCI Machine Learning, [link](https://archive.ics.uci.edu/ml/datasets/Car+Evaluation). We used the `car.data` (a csv)  file, modified it by adding the labels to the first row, so that parsing in the data becomes easy, and renamed it as `car.csv`.\n",
    "The dataset consists of some categorical variables describing used cars, and `label` indicates their current condition. More details of the attributes are available on the dataset page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read in the data below, and develop a general idea of the datatset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"car.csv\")\n",
    "df.head(3) # some sample rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape # 1728 rows, with 6 attributes and 1 label each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns    # attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`label` is our target field. Lets have a look at the kind of labels.\n",
    "\n",
    "`unacc` -> unacceptable  |  `acc`   -> acceptable  |  `vgood` -> very good  |  `good`  -> good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want to use the naive bayes classifier for binary classification and since the number of `unacc` values are very large, we make the choice of bining the `acc`, `good`, `vgood` into a single category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, for convinience, we will encode the label `unacc` as `0` and `acc`, `good`, `vgood` as `1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labels = []\n",
    "for i in df['label']:\n",
    "    if i == 'unacc':\n",
    "        labels.append(0)\n",
    "    else: labels.append(1)\n",
    "    #convert labels datatype into binary(0,1) format\n",
    "    #this is done to easily calculate accuracy and precision\n",
    "\n",
    "labels = pd.DataFrame(labels, columns=['label']) #converting the numpy array to a dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the new target label distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.drop([\"label\"], axis=1) #dropping label column to make input argument for the splitting function\n",
    "features.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We opt for a standard 80-20, train-test split. Since our data is skewed, we do a stratified split so as to ensure an even distribution of +ve and -ve classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels,\n",
    "                                                                            test_size=0.2,\n",
    "                                                                            random_state=0,\n",
    "                                                                            stratify=labels)\n",
    "\n",
    "# resetting indexes so that they are in ascending order\n",
    "train_features.reset_index(drop=True, inplace=True)\n",
    "test_features.reset_index(drop=True, inplace=True)\n",
    "train_labels.reset_index(drop=True, inplace=True)\n",
    "test_labels.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuring we have an even +ve and -ve class distribution. In both train and test sets, we have an approx 2:1 ratio of -ve to +ve classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We join the labels back to our train set, so as to make our algorithm implementation easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = pd.concat([train_features, train_labels], axis=1)\n",
    "train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Naive_Bayes():\n",
    "    def __init__(self, data):\n",
    "        # last col of data is assumed to be the targets\n",
    "        # encoded as 0 or 1\n",
    "        # we will store the -ve and +ve instances separately\n",
    "        self.data = (data[data.iloc[:, -1] == 0], data[data.iloc[:, -1] == 1])\n",
    "        # calculate and store the priors\n",
    "        self.priors = self._calc_priors()\n",
    "        # calculate counts of different values, of different attributes\n",
    "        # separately, for +ve and -ve\n",
    "        # stored as a dict for easy querying\n",
    "        self.counts = self._calc_counts()\n",
    "\n",
    "    \n",
    "    # calculates the prior probabilities ie P(1) and P(0) from the whole dataset\n",
    "    def _calc_priors(self):\n",
    "        total = len(self.data[0]) + len(self.data[1])\n",
    "        pos_prior = len(self.data[1])/total\n",
    "        neg_prior = len(self.data[0])/total\n",
    "        return neg_prior, pos_prior\n",
    "    \n",
    "    # calculates count of occurences, of unique values, of each attribute\n",
    "    # for each class separately\n",
    "    def _calc_counts(self):\n",
    "        result = []\n",
    "        for class_idx in range(2):\n",
    "            temp = {}\n",
    "            data = self.data[class_idx]\n",
    "            for i, col in data.iteritems():\n",
    "                temp[i] = col.value_counts()\n",
    "            result.append(temp)\n",
    "        return tuple(result)\n",
    "    \n",
    "    # calculates probability for both classes, for each row in data\n",
    "    def get_probs(self, data):\n",
    "        # for storing the probabilities of each class, for each data point\n",
    "        result = []\n",
    "        for _, row in data.iterrows():\n",
    "            # for the denominator of the bayes theorem\n",
    "            denom = 1\n",
    "            # will eventually store the actual probabilities\n",
    "            probs = [1, 1]\n",
    "            # iterate over all except label\n",
    "            for attr in self.data[0].columns[0:-1]:\n",
    "                value_counts = [0, 0]\n",
    "                attr_total = [0, 0]\n",
    "                cond_probs = [0, 0]\n",
    "                # for each class, calculate conditional prob of attr\n",
    "                for class_index in range(2):\n",
    "                    if str(row[attr]) in self.counts[class_index][attr]:\n",
    "                        value_counts[class_index] = self.counts[class_index][attr][row[attr]]\n",
    "                    cond_probs[class_index] = value_counts[class_index] / self.data[class_index].shape[0]\n",
    "                    if cond_probs[class_index] == 0:\n",
    "                        cond_probs[class_index] = 1e-9\n",
    "                    probs[class_index] *= (cond_probs[class_index])\n",
    "                # multiplying, naive assumption, all are independent events\n",
    "                denom *= (value_counts[0] + value_counts[1]) / (self.data[0].shape[0] + self.data[1].shape[0])\n",
    "                if denom == 0:\n",
    "                    denom = 1e-9\n",
    "            \n",
    "            for class_index in range(2):\n",
    "                # bayes theorem\n",
    "                probs[class_index] *= (self.priors[class_index]/denom)\n",
    "            \n",
    "            result.append(probs)\n",
    "        return result\n",
    "    \n",
    "    # uesd for prediction, offloads calculation to get_probs\n",
    "    # then checks which class has higher and returns that label\n",
    "    def predict(self, data):\n",
    "        probs = self.get_probs(data)\n",
    "        labels = np.zeros(len(probs))\n",
    "        for i, prob in enumerate(probs):\n",
    "            if prob[0] > prob[1]:\n",
    "                labels[i] = 0\n",
    "            else:\n",
    "                labels[i] = 1\n",
    "        return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll train our classifier on the train data that we separated earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_bayes = Naive_Bayes(train)  #initialize class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll test the model by obtaining predictions on our previously made test set and store the labels in `pred_labels`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels = naive_bayes.predict(test_features)  #initialize method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results and Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our predicted values are skewed, but that is expected as our test data is skewed, similar to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(pred_labels, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a better idea of which cases our model is getting wrong, we'll generate the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat = metrics.confusion_matrix(test_labels, pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for pretty plotting a confusion matrix\n",
    "def plot_conf_matrix(cf):\n",
    "    group_names = ['True Neg','False Pos','False Neg','True Pos']\n",
    "    group_counts = [\"{0:0.0f}\".format(value) for value in\n",
    "                   cf.flatten()]\n",
    "    group_percentages = [\"{0:.2%}\".format(value) for value in\n",
    "                        cf.flatten()/np.sum(cf)]\n",
    "    labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
    "             zip(group_names,group_counts,group_percentages)]\n",
    "    labels = np.asarray(labels).reshape(2,2)\n",
    "    sns.heatmap(cf, annot=labels, fmt='', cmap='viridis')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, our simple model is getting most of the test points correct __Note__: The percentages are based on the total number of test points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_conf_matrix(conf_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of our model is very good considering its a simple naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = metrics.accuracy_score(test_labels, pred_labels)\n",
    "acc   #accuracy measure = true cases/total cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prec = metrics.precision_score(test_labels, pred_labels)\n",
    "prec   #precision measure = true positive/ total positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec = metrics.recall_score(test_labels, pred_labels)\n",
    "rec   #recall measure = true positive/(true positive + false negative) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
