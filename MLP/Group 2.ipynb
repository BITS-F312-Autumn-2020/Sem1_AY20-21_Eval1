{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qoKt3w-6wOlh"
   },
   "outputs": [],
   "source": [
    "#Necessary imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "import random\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "import math\n",
    "import operator\n",
    "from sklearn.metrics import accuracy_score,precision_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.datasets import make_blobs,make_classification\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YPj6jwEww8pU"
   },
   "outputs": [],
   "source": [
    "def train_test_split(df , train_split):\n",
    "    df = df.sample(frac=1).reset_index(drop=True) #randomizing dataset\n",
    "    n = math.ceil((1-train_split)*len(df)) #calculating number of rows for training set\n",
    "    df_train = df.head(n) #training dataset\n",
    "    df_test = df.tail(len(df) - n) #test dataset\n",
    "    return df_train,df_test #returning splitted dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rOXFhwrCwjG-"
   },
   "source": [
    "## HW-1 Perceptron\n",
    "\n",
    "## References:\n",
    "\n",
    "### Dataset:  Synthetic data using sklearn make_classification                                                                                        (Source:  https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html )\n",
    "\n",
    "\n",
    "### Unified Learning Algorithm : Lecture - 2 , Instructor: Basabdatta Sen Bhattacharya , Course:  Neural Networks and Fuzzy Logic (BITS F312) ,  BITS PILANI, GOA CAMPUS\n",
    "\n",
    "### Stochastic Gradient Algorithm : Aishwarya  V Srinivasan , Stochastic Gradient Descent - Clearly Explained (Source:  https://towardsdatascience.com/stochastic-gradient-descent-clearly-explained-53d239905d31 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WZcvYYURzIx6"
   },
   "outputs": [],
   "source": [
    "# Generating Clean data\n",
    "X,y = make_classification(n_samples=10000, n_features=10, n_informative=2,n_redundant=0, n_repeated=0, n_classes=2, n_clusters_per_class=1,class_sep=2,flip_y=0,weights=[0.5,0.5], random_state=17)\n",
    "df = pd.DataFrame(data = X)\n",
    "df['Output'] = y\n",
    "df.to_csv('dataset.csv' , index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 543
    },
    "colab_type": "code",
    "id": "1JDlkndcb4zR",
    "outputId": "6ca0120b-cb85-4047-bcac-2ec000aed074"
   },
   "outputs": [],
   "source": [
    "if X.shape[1]>2:\n",
    "    pca = PCA(n_components=2)\n",
    "    plotX=pca.fit_transform(X)\n",
    "else:\n",
    "    plotX=X\n",
    "f,ax1 = plt.subplots(nrows=1, ncols=1,figsize=(10,8))\n",
    "sns.scatterplot(plotX[:,0],plotX[:,1],hue=y,ax=ax1);\n",
    "ax1.set_title(\"First two Principal Componenets of the Data-set\",fontsize=20)\n",
    "ax1.set_xlabel(\"First Dimension of the data\", fontsize = 18)\n",
    "ax1.set_ylabel(\"Second Dimension of the data\", fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "prFLw2ey3Ra4"
   },
   "outputs": [],
   "source": [
    "def activation_function(z):\n",
    "    if(z>=0):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def activation_function_sgd(z):\n",
    "    return 1/(1+math.exp(-z))\n",
    "\n",
    "def get_y(z):\n",
    "    if(z>=0.5):\n",
    "        return 1.0\n",
    "    else:\n",
    "        return 0.0\n",
    "    \n",
    "def rmse(predictions, targets): \n",
    "    return np.sqrt(((predictions - targets) ** 2).mean())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k0ajVjKKauK3"
   },
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    def __init__(self,dimension):\n",
    "        self.n = dimension\n",
    "        self.W = np.random.rand(self.n)#assigning random weights initially\n",
    "        self.bias = random.random() #assigning random bias value\n",
    "    def predict(self, X_test ):\n",
    "        predict_outputs = []\n",
    "        for i in range(len(X_test)):\n",
    "            z = np.dot(self.W,X_test.iloc[i].values) # we calculate the sum corresponding the weights\n",
    "            z = z + self.bias #adding bias since not considered in above step\n",
    "        \n",
    "            predict = activation_function(z) #this is our predicted output\n",
    "            predict_outputs.append(get_y(predict))\n",
    "        return predict_outputs\n",
    "\n",
    "    def unified_training(self,X_train , y_train , epochs = 100):        \n",
    "        start = time.time()        \n",
    "        error_list = []\n",
    "        acc_list = []\n",
    "        X_train_copy = X_train.copy(deep=True)\n",
    "        y_train_copy = y_train.copy(deep=True)\n",
    "        for j in range(epochs):\n",
    "            #we need to train our model now using these weights\n",
    "            for i in range(len(X_train_copy)):\n",
    "                X = X_train_copy.iloc[i].values #input is in X\n",
    "                z = np.dot(self.W,X) # we calculate the sum corresponding the weights\n",
    "                z = z + self.bias #adding bias since not considered in above step\n",
    "\n",
    "                predict_ = activation_function(z) #this is our predicted output\n",
    "                target = y_train_copy[i] # correct output corresponding to the input in the current loop\n",
    "                \n",
    "                error = target-predict_ #calculating error for the given input\n",
    "                self.W = self.W + error * X  #changing the weights as per the error\n",
    "                self.bias = self.bias + error                  #changing bias as per the above weights\n",
    "                \n",
    "                    \n",
    "            predict_outputs = self.predict(X_train)\n",
    "            cal1 = rmse(predict_outputs , y_train)\n",
    "            cal2 = accuracy_score(predict_outputs , y_train)\n",
    "            print(\"Epoch : \" , j , \"Error : \" , cal1 , \"Accuracy : \" , cal2)\n",
    "            error_list.append(cal1)\n",
    "            acc_list.append(cal2)\n",
    "            \n",
    "        end = time.time()\n",
    "        time_taken = end-start\n",
    "        return error_list,acc_list,time_taken\n",
    "            \n",
    "    def stochastic_gradient(self,X_train , y_train , learning_rate = 0.1 , epochs = 100):    \n",
    "        start = time.time()\n",
    "        error_list = []\n",
    "        acc_list = []        \n",
    "        X_train_copy = X_train.copy(deep=True)\n",
    "        y_train_copy = y_train.copy(deep=True)\n",
    "        #training the neural network using stochastic gradient\n",
    "        for j in range(epochs):\n",
    "            X_train_copy[\"Output\"] = y_train_copy\n",
    "            X_train_copy = X_train_copy.sample(frac=1).reset_index(drop=True)\n",
    "            y_train_copy = X_train_copy[\"Output\"]\n",
    "            X_train_copy.drop(\"Output\",axis=1,inplace=True)\n",
    "            for i in range(len(X_train_copy)) : \n",
    "                #input_index = random.randint(0,no_of_inputs-1) #taking random input\n",
    "                \n",
    "                X = X_train_copy.iloc[i].values #input is in X\n",
    "                z = np.dot(self.W,X) #took the summation over all features\n",
    "                z = z + self.bias #adding bias since not considered above\n",
    "                target = y_train_copy[i]\n",
    "                \n",
    "                sigma = activation_function_sgd(z) #applying activation function on summation\n",
    "                y = get_y(sigma) #getting the predicted value from the current weights\n",
    "                #update weights as per the error\n",
    "                self.W = self.W - ((learning_rate)*(target - y)*sigma*(sigma-1)*X)\n",
    "                self.bias = self.bias - ((learning_rate)*(target-y)*sigma*(sigma-1))\n",
    "\n",
    "            predict_outputs = self.predict(X_train)\n",
    "            cal1 = rmse(predict_outputs , y_train)\n",
    "            cal2 = accuracy_score(predict_outputs , y_train)\n",
    "            print(\"Epoch : \" , j , \"Error :\" , cal1 , \"Accuracy : \" , cal2)\n",
    "            error_list.append(cal1)\n",
    "            acc_list.append(cal2)        \n",
    "\n",
    "        end = time.time()\n",
    "        time_taken = end-start\n",
    "        return error_list,acc_list,time_taken    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zoCTCX8yauUP"
   },
   "outputs": [],
   "source": [
    "#preprocess data\n",
    "df = pd.read_csv(\"dataset.csv\") #reading the csv file\n",
    "df = df.sample(frac=1).reset_index(drop=True) #randomizing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OG3XqdjzauQq"
   },
   "outputs": [],
   "source": [
    "#calculate features that we require for training\n",
    "features = df.columns.values.tolist() # all features or inputs that we have\n",
    "features.remove(\"Output\") #we have to predict TARGET_5Yrs so removing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "colab_type": "code",
    "id": "3cOFjq5wauHw",
    "outputId": "110795b1-eafd-478d-aee8-774303a59734"
   },
   "outputs": [],
   "source": [
    "### normalizing the dataset given using Standard Scaler ###\n",
    "\n",
    "scaler=StandardScaler()\n",
    "scaled_data=scaler.fit(df[features]).transform(df[features])\n",
    "df[features]=pd.DataFrame(scaled_data,columns=features)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S_ZFOkMwauEe"
   },
   "outputs": [],
   "source": [
    "new_split_ratio = 0.4 #to check the effect of split ratio we will use this later on\n",
    "new_learning_rate = 0.2 #to check the effect of learning rate we will use this later on\n",
    "epochs = 10 #number of epochs to run for all training algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CMWXXU4fauBh"
   },
   "outputs": [],
   "source": [
    "df_train,df_test = train_test_split(df , 0.2)\n",
    "df_train_split,df_test_split = train_test_split(df , new_split_ratio)\n",
    "df_train_split = df_train.reset_index(drop=True)\n",
    "df_test_split = df_test.reset_index(drop=True)\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_test = df_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S-00J9P6kkJg"
   },
   "outputs": [],
   "source": [
    "y_train = df_train[\"Output\"] #this is our training prediction parameter\n",
    "y_test = df_test[\"Output\"] #keeping so that we can check accuracy of our model\n",
    "X_train = df_train[features] #this becomes our training dataset\n",
    "X_test = df_test[features] #this becomes our test dataset\n",
    "\n",
    "y_train_split = df_train_split[\"Output\"] #this is our training prediction parameter\n",
    "y_test_split = df_test_split[\"Output\"] #keeping so that we can check accuracy of our model\n",
    "X_train_split = df_train_split[features] #this becomes our training dataset\n",
    "X_test_split = df_test_split[features] #this becomes our test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 353
    },
    "colab_type": "code",
    "id": "IxavWOUIkkSR",
    "outputId": "166d17c6-af79-4b6d-fdee-f5e1813788e6"
   },
   "outputs": [],
   "source": [
    "### unified algorithm training ###\n",
    "perceptron1 = Perceptron(X_train.shape[1])\n",
    "error_list_unified,acc_list_unified,time_taken_unified = perceptron1.unified_training(X_train , y_train , epochs = epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 353
    },
    "colab_type": "code",
    "id": "4GXR3v6kkkab",
    "outputId": "7aac8576-14fb-4822-ac35-9b90c552e991"
   },
   "outputs": [],
   "source": [
    "### training using stochastic gradient descent algorithm with learning rate = 0.2 ###\n",
    "perceptron2 = Perceptron(X_train.shape[1])\n",
    "error_list_sgd,acc_list_sgd,time_taken_sgd = perceptron2.stochastic_gradient(X_train , y_train , learning_rate = 0.005 , epochs = epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "MM774jYdkkjo",
    "outputId": "e3522a71-fd9d-4187-863e-1c159d1c066b"
   },
   "outputs": [],
   "source": [
    "#print error curve for both type of training\n",
    "x = list(range(0,epochs))\n",
    "plt.plot(x,error_list_unified,label='Unified training error')\n",
    "plt.plot(x,error_list_sgd,label='Stochastic gradient error')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Respective errors')\n",
    "plt.title('Training comparison in terms of error')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "Dnf4SCNWkkwk",
    "outputId": "9882f57c-ae74-4a7e-f9cb-1712a54fafd4"
   },
   "outputs": [],
   "source": [
    "#print accuracy curve for both type of training\n",
    "plt.plot(x,acc_list_unified,label='Unified training accuracy')\n",
    "plt.plot(x,acc_list_sgd,label='Stochastic gradient accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Respective accuracy')\n",
    "plt.title('Training comparison in terms of accuracy on training dataset')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "KOTt1AY4kktA",
    "outputId": "698f0478-d093-487f-bed9-075ec2917dba"
   },
   "outputs": [],
   "source": [
    "#comparing algorithm based on their time stamps with equal parameters\n",
    "print(\"Time taken by following algorithms in seconds : \")\n",
    "print(\"Unified_training    : \" , time_taken_unified)\n",
    "print(\"Stochastic gradient : \" , time_taken_sgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 353
    },
    "colab_type": "code",
    "id": "M9U41s0RkkgP",
    "outputId": "b78768fe-386f-48ae-ec61-40ea5e1ec74a"
   },
   "outputs": [],
   "source": [
    "#seeing how sgd get affected by changing learning rate\n",
    "perceptron3  = Perceptron(X_train.shape[1])\n",
    "error_list_sgd_new,acc_list_sgd_new,time_taken_sgd_new = perceptron3.stochastic_gradient(X_train , y_train , \n",
    "                                                        learning_rate = new_learning_rate , epochs = epochs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "5eSfPfltkkXD",
    "outputId": "4f94b5c0-e8a4-4f14-fe1b-09718de1e465"
   },
   "outputs": [],
   "source": [
    "x = list(range(0,epochs))\n",
    "plt.plot(x,error_list_sgd,label='SGD training error, lr = 0.005')\n",
    "plt.plot(x,error_list_sgd_new,label='SGD training error , lr = '+ str(new_learning_rate))\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Respective errors')\n",
    "plt.title('Training comparison in terms of error')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "aPClaEu7kkO4",
    "outputId": "0b5eacd2-da69-4fb4-c4e2-5c2ecfedfffb"
   },
   "outputs": [],
   "source": [
    "#seeing how sgd_acc get affected by changing learning rate\n",
    "plt.plot(x,acc_list_sgd,label='SGD training accuracy, lr = 0.005')\n",
    "plt.plot(x,acc_list_sgd_new,label='SGD training accuracy , lr = '+ str(new_learning_rate))\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Respective accuracy')\n",
    "plt.title('Training comparison in terms of accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 353
    },
    "colab_type": "code",
    "id": "BwUnw4ixkkG6",
    "outputId": "bf4bbe9a-4e5f-49ed-aa6c-a5e7baa1cd4d"
   },
   "outputs": [],
   "source": [
    "# seeing the effect of test train split on varying split ratio\n",
    "perceptron4 = Perceptron(X_train.shape[1])\n",
    "error_list_sgd_split,acc_list_sgd_split,time_taken_sgd_split = perceptron4.stochastic_gradient(X_train_split , y_train_split , learning_rate = 0.005 , epochs =epochs )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "uG5qeo7vrCjq",
    "outputId": "20272669-6e9a-4f93-d41e-8d3f8d2c98cb"
   },
   "outputs": [],
   "source": [
    "#seeing how sgd_error get affected by changing split ratio\n",
    "x = list(range(0,epochs))\n",
    "plt.plot(x,error_list_sgd,label='SGD training error, split = 0.2')\n",
    "plt.plot(x,error_list_sgd_split,label='SGD training error , split = '+ str(new_split_ratio))\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Respective errors')\n",
    "plt.title('Training comparison in terms of error')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "sjH-c9CIrCgW",
    "outputId": "b9aed724-e754-4320-9bbc-494d916c28b1"
   },
   "outputs": [],
   "source": [
    "#seeing how sgd_acc get affected by changing split ratio\n",
    "plt.plot(x,acc_list_sgd,label='SGD training accuracy, split = 0.2')\n",
    "plt.plot(x,acc_list_sgd_split,label='SGD training accuracy , split = '+ str(new_split_ratio))\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Respective accuracy')\n",
    "plt.title('Training comparison in terms of accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N4ek48Zzr0hr"
   },
   "outputs": [],
   "source": [
    "#predicting values on X_test for each training algorithm\n",
    "predict_xtest_unified = perceptron1.predict(X_test )\n",
    "predict_xtest_sgd = perceptron2.predict(X_test )\n",
    "predict_xtest_sgd_new = perceptron3.predict(X_test)\n",
    "predict_xtest_sgd_split = perceptron4.predict(X_test_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "colab_type": "code",
    "id": "-pePzLhhr2F3",
    "outputId": "36c7d716-66db-4d29-e831-c29cc0441302"
   },
   "outputs": [],
   "source": [
    "#print confusion matrix for both type of training\n",
    "\n",
    "\n",
    "cf_matrix_unified = confusion_matrix(y_test , predict_xtest_unified)\n",
    "cf_matrix_sgd = confusion_matrix(y_test , predict_xtest_sgd)\n",
    "cf_matrix_sgd_new = confusion_matrix(y_test , predict_xtest_sgd_new)\n",
    "cf_matrix_sgd_split = confusion_matrix(y_test_split, predict_xtest_sgd_split)\n",
    "\n",
    "\n",
    "group_names = ['True Neg','False Pos','False Neg','True Pos']\n",
    "group_counts = [\"{0:0.0f}\".format(value) for value in\n",
    "                cf_matrix_unified.flatten()]\n",
    "group_percentages = [\"{0:.2%}\".format(value) for value in\n",
    "                     cf_matrix_unified.flatten()/np.sum(cf_matrix_unified)]\n",
    "labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
    "          zip(group_names,group_counts,group_percentages)]\n",
    "labels = np.asarray(labels).reshape(2,2)\n",
    "\n",
    "sns.heatmap(cf_matrix_unified, annot=labels, fmt='', cmap='Blues')\n",
    "plt.xlabel(\"Predicted values\")\n",
    "plt.ylabel(\"Actual values\")\n",
    "plt.title(\"Heatmap for unified learning\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "colab_type": "code",
    "id": "B7gk3U1Qr2CP",
    "outputId": "37a0fb4f-8a1f-4521-99e1-b0720f26f1ee"
   },
   "outputs": [],
   "source": [
    "group_names = ['True Neg','False Pos','False Neg','True Pos']\n",
    "group_counts = [\"{0:0.0f}\".format(value) for value in\n",
    "                cf_matrix_sgd.flatten()]\n",
    "group_percentages = [\"{0:.2%}\".format(value) for value in\n",
    "                     cf_matrix_sgd.flatten()/np.sum(cf_matrix_sgd)]\n",
    "labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
    "          zip(group_names,group_counts,group_percentages)]\n",
    "labels = np.asarray(labels).reshape(2,2)\n",
    "\n",
    "sns.heatmap(cf_matrix_sgd, annot=labels, fmt='', cmap='Blues')\n",
    "plt.xlabel(\"Predicted values\")\n",
    "plt.ylabel(\"Actual values\")\n",
    "plt.title(\"Heatmap for stochastic gradient learning with lr = 0.005\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "colab_type": "code",
    "id": "RqmcZTV-sTck",
    "outputId": "c0b242e0-d38c-4770-9a07-4b41b48947cf"
   },
   "outputs": [],
   "source": [
    "group_names = ['True Neg','False Pos','False Neg','True Pos']\n",
    "group_counts = [\"{0:0.0f}\".format(value) for value in\n",
    "                cf_matrix_sgd_new.flatten()]\n",
    "group_percentages = [\"{0:.2%}\".format(value) for value in\n",
    "                     cf_matrix_sgd_new.flatten()/np.sum(cf_matrix_sgd_new)]\n",
    "labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
    "          zip(group_names,group_counts,group_percentages)]\n",
    "labels = np.asarray(labels).reshape(2,2)\n",
    "\n",
    "sns.heatmap(cf_matrix_sgd_new, annot=labels, fmt='', cmap='Blues')\n",
    "plt.xlabel(\"Predicted values\")\n",
    "plt.ylabel(\"Actual values\")\n",
    "plt.title(\"Heatmap for stochastic gradient learning with lr =\" + str(new_learning_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "colab_type": "code",
    "id": "bN5qhz5AsT3i",
    "outputId": "d9c4128a-c291-4944-9904-38579fcd105a"
   },
   "outputs": [],
   "source": [
    "group_names = ['True Neg','False Pos','False Neg','True Pos']\n",
    "group_counts = [\"{0:0.0f}\".format(value) for value in\n",
    "                cf_matrix_sgd_split.flatten()]\n",
    "group_percentages = [\"{0:.2%}\".format(value) for value in\n",
    "                     cf_matrix_sgd_split.flatten()/np.sum(cf_matrix_sgd_split)]\n",
    "labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
    "          zip(group_names,group_counts,group_percentages)]\n",
    "labels = np.asarray(labels).reshape(2,2)\n",
    "\n",
    "sns.heatmap(cf_matrix_sgd_split, annot=labels, fmt='', cmap='Blues')\n",
    "plt.xlabel(\"Predicted values\")\n",
    "plt.ylabel(\"Actual values\")\n",
    "plt.title(\"Heatmap for stochastic gradient learning with split ratio =\" + str(new_split_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "id": "vX7WBZs1sTvO",
    "outputId": "e9043f34-c7de-4d98-fd11-52291825b7ac"
   },
   "outputs": [],
   "source": [
    "precision_unified = precision_score(predict_xtest_unified , y_test)\n",
    "precision_sgd = precision_score(predict_xtest_sgd , y_test)\n",
    "precision_sgd_new = precision_score(predict_xtest_sgd_new , y_test)\n",
    "precision_sgd_split = precision_score(predict_xtest_sgd_split , y_test_split)\n",
    "\n",
    "accuracy_unified = accuracy_score(predict_xtest_unified , y_test)\n",
    "accuracy_sgd = accuracy_score(predict_xtest_sgd , y_test)\n",
    "accuracy_sgd_new = accuracy_score(predict_xtest_sgd_new , y_test)\n",
    "accuracy_sgd_split = accuracy_score(predict_xtest_sgd_split , y_test_split)\n",
    "\n",
    "print(\"TEST DATASET RESULTS : \")\n",
    "print()\n",
    "\n",
    "print(\"Following are precisions in different cases :\")\n",
    "print()\n",
    "\n",
    "print(\"Precision in unified learning                   : \" , precision_unified)\n",
    "print(\"Precision in sgd learning with lr 0.005         : \" , precision_sgd)\n",
    "print(\"Precision in sgd learning with lr \",new_learning_rate,\"         : \" , precision_sgd_new)\n",
    "print(\"Precision in sgd learning with split ratio \",new_split_ratio,\": \" , precision_sgd_split)\n",
    "print()\n",
    "\n",
    "print(\"Following are accuracies in different cases :\")\n",
    "print()\n",
    "print(\"Accuracy in unified learning                   : \" , accuracy_unified)\n",
    "print(\"Accuracy in sgd learning with lr 0.005         : \" , accuracy_sgd)\n",
    "print(\"Accuracy in sgd learning with lr\",new_learning_rate,\"          : \" , accuracy_sgd_new)\n",
    "print(\"Accuracy in sgd learning with split ratio \",new_split_ratio,\": \" , accuracy_sgd_split)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NNFL_T1_G2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
