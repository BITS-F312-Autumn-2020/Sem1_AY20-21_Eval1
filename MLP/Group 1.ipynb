{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/coding-neural-network-forward-propagation-and-backpropagtion-ccf8cf369f76\n",
    "# https://brilliant.org/wiki/backpropagation/\n",
    "# https://www.kaggle.com/uciml/breast-cancer-wisconsin-data/download/sZAHVX74GzL8DkikkBxh%2Fversions%2FYKMwG6P7QgYwcBBhHi0D%2Ffiles%2Fdata.csv\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns # for pretty plots\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split \n",
    "#from sklearn import metrics #for making the confusion matrix and finding the accuracy of the model\n",
    "import gc # to manually activate gc, for more accurate time measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"data.csv\")\n",
    "#df stores the raw data from our breast cancer dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=[]\n",
    "for i in df[\"diagnosis\"]:\n",
    "    if i==\"M\":\n",
    "        Y.append(1.0)\n",
    "    else:\n",
    "        Y.append(0.0)\n",
    "        \n",
    "#Y -> stores whether the person has malignant or benign tumor\n",
    "#Y -> 1 for malignant tumor\n",
    "#  -> 0 for benign tumor\n",
    "#shape of Y is (569,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=pd.DataFrame(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop([\"diagnosis\",\"id\",\"Unnamed: 32\"],axis=1) \n",
    "#removing unnecessary columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train & Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df, Y, test_size = 0.2, random_state = 0)\n",
    "# X_train-> training dataset inputs\n",
    "# y_train-> training dataset labels\n",
    "# X_test-> testing dataset inputs\n",
    "# y_test-> testing dataset labels\n",
    "# the train-test split size 80%-20% "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.reset_index(drop=True,inplace=True)\n",
    "X_test.reset_index(drop=True,inplace=True)\n",
    "y_train.reset_index(drop=True,inplace=True)\n",
    "y_test.reset_index(drop=True,inplace=True)\n",
    "# to rest the indexes of the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Helper(object):    \n",
    "    def initialize_parameters(self,layers_dims): # initializes the weights and biases for all the layers\n",
    "        np.random.seed(1)               \n",
    "        parameters = {} #parameter is a dictionary dictionary which stores the randomly initialized weights and bias matrix of all the layers\n",
    "        L = len(layers_dims) #L is no of layers           \n",
    "\n",
    "        for l in range(1, L):           \n",
    "            parameters[\"W\" + str(l)] = np.random.randn(\n",
    "                layers_dims[l], layers_dims[l - 1]) * 0.01  # multiplird by 0.01 so that activation functions work in activated mode\n",
    "            parameters[\"b\" + str(l)] = np.zeros((layers_dims[l], 1))\n",
    "\n",
    "        #asserting to check whether the size of the weights and bias matrix is valid\n",
    "        assert parameters[\"W\" + str(l)].shape == (\n",
    "            layers_dims[l], layers_dims[l - 1])\n",
    "        assert parameters[\"b\" + str(l)].shape == (layers_dims[l], 1)\n",
    "    \n",
    "        return parameters\n",
    "\n",
    "    def sigmoid(self,Z):\n",
    "        A = 1 / (1 + np.exp(-Z))\n",
    "        return A, Z\n",
    "\n",
    "\n",
    "    def linear_forward(self,A_prev, W, b):\n",
    "        Z = np.dot(W, A_prev) + b \n",
    "        cache = (A_prev, W, b)\n",
    "        return Z, cache\n",
    "\n",
    "\n",
    "    def linear_activation_forward(self,A_prev, W, b):\n",
    "\n",
    "        Z, linear_cache = self.linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = self.sigmoid(Z)\n",
    "        assert A.shape == (W.shape[0], A_prev.shape[1])\n",
    "        # linear cache stores (A_prev,W,b) for that layer and activation cache stores the Z for that layer. This cache dictionary will\n",
    "        # be later used in the backpropogation part of the code for calculating derivatives.\n",
    "        cache = (linear_cache, activation_cache)\n",
    "        return A, cache\n",
    "\n",
    "\n",
    "    def L_model_forward(self,X, parameters):\n",
    "        A = X.T                          \n",
    "        caches = []                     \n",
    "        L = len(parameters) // 2        \n",
    "    \n",
    "        #we iterate over layers 1 to total layers-1\n",
    "        for l in range(1, L):\n",
    "            A_prev = A\n",
    "            A, cache = self.linear_activation_forward(\n",
    "            A_prev, parameters[\"W\" + str(l)], parameters[\"b\" + str(l)])\n",
    "            caches.append(cache)\n",
    "        \n",
    "        #iterating over the last layer\n",
    "        AL, cache = self.linear_activation_forward(A, parameters[\"W\" + str(L)], parameters[\"b\" + str(L)]) #AL stores the output of the final layer\n",
    "        caches.append(cache)\n",
    "        \n",
    "        assert AL.shape == (1, X.shape[0])\n",
    "        return AL, caches\n",
    "        \n",
    "        \n",
    "    \n",
    "    def compute_cost(self,AL, y,cost_type): #cost_type is for choosing between the cross extropy and sum of square error\n",
    "        m = y.shape[1] \n",
    "        #cost_type=0->cross entropy loss\n",
    "        #cost_type=1->sum of square error\n",
    "        if cost_type==0:\n",
    "            cost = - (1 / m) * np.sum(np.multiply(y, np.log(AL)) + np.multiply(1 - y, np.log(1 - AL)))\n",
    "        else:\n",
    "            cost=1/(m)* np.sum((y-AL)**2)\n",
    "        return cost\n",
    "    \n",
    "    \n",
    "    def sigmoid_gradient(self,dA, Z):\n",
    "        A, Z = self.sigmoid(Z)\n",
    "        dZ = dA * A * (1 - A)\n",
    "        return dZ\n",
    "    \n",
    "    # function to calculate the derivative of the cost function with respect to weight,bias,activation\n",
    "    def linear_backword(self,dZ, cache): \n",
    "        A_prev, W, b = cache\n",
    "        m = A_prev.shape[1]\n",
    "\n",
    "        dW = (1 / m) * np.dot(dZ, A_prev.T) #derivative of cost function w.r.t to weight\n",
    "        db = (1 / m) * np.sum(dZ, axis=1, keepdims=True) #derivative of cost function w.r.t to bias\n",
    "        dA_prev = np.dot(W.T, dZ) #derivative of cost function w.r.t to activation function[sigmoid of z]\n",
    "\n",
    "        assert dA_prev.shape == A_prev.shape\n",
    "        assert dW.shape == W.shape\n",
    "        assert db.shape == b.shape\n",
    "\n",
    "        return dA_prev, dW, db\n",
    "\n",
    "\n",
    "    def linear_activation_backward(self,dA, cache):\n",
    "        linear_cache, activation_cache = cache\n",
    "        dZ = self.sigmoid_gradient(dA, activation_cache)\n",
    "        dA_prev, dW, db = self.linear_backword(dZ, linear_cache)\n",
    "\n",
    "        return dA_prev, dW, db\n",
    "\n",
    "    #function to perform the backpropogation\n",
    "    def L_model_backward(self,AL, y, caches,cost_type):\n",
    "        y = y.reshape(AL.shape)\n",
    "        L = len(caches)\n",
    "        # grads is a dictionary to store the dA,dW,db for each layer. This dictionary will be used to update the parameters.\n",
    "        grads = {}\n",
    "\n",
    "        #cost_type=0-> cross entropy loss\n",
    "        #cost_type=1-> sum of square error\n",
    "        if cost_type==0:\n",
    "            dAL = np.divide(AL - y, np.multiply(AL, 1 - AL))\n",
    "        else:\n",
    "            dAL= 2*(AL-y)\n",
    "\n",
    "        #calculating dA,dW,dB for the final layer\n",
    "        grads[\"dA\" + str(L - 1)], grads[\"dW\" + str(L)], grads[\n",
    "        \"db\" + str(L)] = self.linear_activation_backward(dAL, caches[L - 1])\n",
    "\n",
    "        #calculating dA,dW,dB for the layers 1 to L-1(in reverse order->backpropogating)\n",
    "        for l in range(L - 1, 0, -1):\n",
    "            current_cache = caches[l - 1]\n",
    "            grads[\"dA\" + str(l - 1)], grads[\"dW\" + str(l)], grads[\n",
    "            \"db\" + str(l)] = self.linear_activation_backward(grads[\"dA\" + str(l)], current_cache)\n",
    "\n",
    "        return grads\n",
    "    \n",
    "    \n",
    "    #function to update the parameters\n",
    "    def update_parameters(self,parameters, grads, learning_rate):\n",
    "        L = len(parameters) // 2\n",
    "\n",
    "        for l in range(1, L + 1):\n",
    "            #updating weights\n",
    "            parameters[\"W\" + str(l)] = parameters[\"W\" + str(l)] - learning_rate * grads[\"dW\" + str(l)]\n",
    "            #updating bias\n",
    "            parameters[\"b\" + str(l)] = parameters[\"b\" + str(l)] - learning_rate * grads[\"db\" + str(l)]\n",
    "        return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the multi-layer model using all the helper functions we wrote before\n",
    "class MLP(object):\n",
    "    #PARAMETERS\n",
    "    #self -> the object being created\n",
    "    #mode -> to choose between batch gradient descent------>  mode=0\n",
    "    #                          stochastic gradient descent(online method)------> mode=1   \n",
    "    #cost_type -> to choose between cross entropy loss------>  cost_type=0\n",
    "    #                               mean squared error------>  cost_type=1\n",
    "    #X -> the training set input features\n",
    "    #y -> the training set input labels\n",
    "    #layers_dims -> dictionary containing number of neurons in each layer\n",
    "    #num_iterations -> epoch\n",
    "    \n",
    "    def L_layer_model(self, mode, cost_type,\n",
    "        X, y, layers_dims, learning_rate=0.01, num_iterations=3000,\n",
    "        print_cost=True,print_graph=True):\n",
    "        np.random.seed(1)\n",
    "\n",
    "        # initialize parameters\n",
    "        h1 = Helper()\n",
    "        parameters = h1.initialize_parameters(layers_dims)\n",
    "        #h1.parameters = initialize_parameters(layers_dims)\n",
    "\n",
    "        # intialize cost list\n",
    "        cost_list = []\n",
    "\n",
    "        # mode=0 ----> batch gradient descent\n",
    "        # mode=1 ----> stochastic greadient descent(online gradient descent)\n",
    "        if(mode==0):\n",
    "            for i in range(num_iterations):\n",
    "                # iterate over L-layers to get the final output and the cache\n",
    "                AL, caches = h1.L_model_forward(X, parameters)\n",
    "\n",
    "                # compute cost to plot it\n",
    "                cost = h1.compute_cost(AL.T, y,cost_type)\n",
    "\n",
    "                # iterate over L-layers backward to get gradients\n",
    "                grads = h1.L_model_backward(AL, y.to_numpy(), caches,cost_type)\n",
    "\n",
    "                # update parameters\n",
    "                parameters = h1.update_parameters(parameters, grads, learning_rate)\n",
    "\n",
    "                # append each 100th cost to the cost list\n",
    "                if (i + 1) % 100 == 0 and print_cost:\n",
    "                #print(f\"The cost after {i + 1} iterations is:\" cost)\n",
    "                    print(\"iteration :\",i,\"cost:\",cost)\n",
    "\n",
    "                if i % 100 == 0:\n",
    "                    cost_list.append(cost)\n",
    "        else:\n",
    "            for i in range(num_iterations):\n",
    "                cost_av = 0\n",
    "                # iterate over L-layers to get the final output and the cache\n",
    "                for j in range(len(X)):\n",
    "                    AL, caches = h1.L_model_forward(X.iloc[j].to_frame().T, parameters)\n",
    "\n",
    "                    # compute cost to plot it\n",
    "                    cost = h1.compute_cost(AL.T, y.iloc[j].to_frame().T,cost_type)\n",
    "                    cost_av = cost_av+cost\n",
    "                    # iterate over L-layers backward to get gradients\n",
    "                    grads = h1.L_model_backward(AL, y.iloc[j].to_frame().T.to_numpy(), caches,cost_type)\n",
    "\n",
    "                    # update parameters\n",
    "                    parameters = h1.update_parameters(parameters, grads, learning_rate)\n",
    "\n",
    "                    # append each 100th cost to the cost list\n",
    "                if (i + 1) % 100 == 0 and print_cost:\n",
    "#                     print(f\"The cost after {i + 1} iterations is:\" cost)\n",
    "                    cost_av = cost_av/len(X)\n",
    "                    print(\"iteration :\",i,\"cost:\",cost_av)\n",
    "\n",
    "                #if i % 100 == 0:\n",
    "                cost_list.append(cost_av)\n",
    "                \n",
    "        # plot the cost curve\n",
    "        if(print_graph==True):\n",
    "            %matplotlib notebook\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(cost_list)\n",
    "            plt.xlabel(\"Iterations (per hundreds)\")\n",
    "            plt.ylabel(\"Loss\")\n",
    "            plt.title(f\"Loss curve for the learning rate = {learning_rate}\")\n",
    "        return parameters    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to find the accuracy of the model\n",
    "def accuracy(X, parameters, y):\n",
    "    h1 = Helper()\n",
    "    probs, caches = h1.L_model_forward(X, parameters)\n",
    "    labels = (probs >= 0.5) * 1\n",
    "    acc = np.mean(labels == y) * 100\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of neurons in each layer\n",
    "layers_dims = [X_train.shape[1], 5, 3, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A model using batch gradient descent and cross entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mlp_batch is an object of the MLP class\n",
    "mlp_batch = MLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par = mlp_batch.L_layer_model(0,0,X_train, y_train, layers_dims, learning_rate=0.03, num_iterations=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(X_test,par, y_test.to_numpy().T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The accuracy for the batch gradient descent model is 86.84% for 3000 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing prediction time for different epochs for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#storing prediction time and accuracy for different epochs(for making graph to compare the computational efficieny of batch and online gradient descent)\n",
    "gc.disable()\n",
    "predictionTime_batch=[]\n",
    "accuracy_batch=[]\n",
    "for i in range(30,301,30):\n",
    "    gc.collect()\n",
    "    startTime = time.process_time()\n",
    "    par = mlp_batch.L_layer_model(0,0,X_train, y_train, layers_dims, learning_rate=0.03, num_iterations=i,print_cost=False,print_graph=False)\n",
    "    endTime = time.process_time()\n",
    "    predictionTime=endTime - startTime\n",
    "    predictionTime_batch.append(predictionTime)\n",
    "    acc=accuracy(X_test,par, y_test.to_numpy().T)\n",
    "    accuracy_batch.append(acc)\n",
    "    gc.collect()\n",
    "gc.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictionTime_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A model using stochastic gradient descent(online) and cross entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_stochastic = MLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par = mlp_stochastic.L_layer_model(1,0,X_train, y_train, layers_dims, learning_rate=0.03, num_iterations=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(X_test,par, y_test.to_numpy().T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The accuracy for the online gradient descent model is 58.77% for 30 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing prediction time for different epochs for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.disable()\n",
    "predictionTime_online=[]\n",
    "accuracy_online=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#storing prediction time and accuracy for different epochs(for making graph to compare the computational efficieny of batch and online gradient descent)\n",
    "for i in range(30,301,30):\n",
    "    gc.collect()\n",
    "    startTime = time.process_time()\n",
    "    par = mlp_stochastic.L_layer_model(1,0,X_train, y_train, layers_dims, learning_rate=0.03, num_iterations=i,print_cost=False,print_graph=False)\n",
    "    endTime = time.process_time()\n",
    "    predictionTime=endTime - startTime\n",
    "    predictionTime_online.append(predictionTime)\n",
    "    acc=accuracy(X_test,par, y_test.to_numpy().T)\n",
    "    accuracy_online.append(acc)\n",
    "    gc.collect()\n",
    "gc.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionTime_online"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph of Batch vs Online Learning Method for comparing computational accuracy(accuracy vs prediction time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x-axis points for the graph\n",
    "x=[]\n",
    "for i in range(30,301,30):\n",
    "    x.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x, predictionTime_online)\n",
    "plt.plot(x, predictionTime_batch)\n",
    "plt.xlabel(\"Number of iterations\")\n",
    "plt.ylabel(\"Prediction Time\")\n",
    "plt.title(f\"Number of iteration VS Prediction Time for Batch and uniform Gradient learning\")\n",
    "plt.legend([\"Online Gradient Descent\" , \"Batch Gradient Descent\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the above graph, we can see that for training the dataset on batch gradient descent(we get almost a strainght line) the prediction time is less as compared to online learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A model using batch gradient descent(online) and mean square error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#mlp_batch is an object of the MLP class\n",
    "mlp_mse = MLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par = mlp_mse.L_layer_model(0,1,X_train, y_train, layers_dims, learning_rate=0.03, num_iterations=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "accuracy(X_test,par, y_test.to_numpy().T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The accuracy for the batch gradient descent model usine mean square error is 58.77% for 3000 epochs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
